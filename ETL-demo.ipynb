{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d1e5cba5-dff1-4657-83ff-cefaeb9fae98",
   "metadata": {},
   "source": [
    "## Environment setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ce7405-f18f-4bc2-b891-c6fa41c19fa7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -U -q sagemaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b4f968-9079-4ccc-aaf6-219aa8956619",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(action=\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8368755c-7d27-41bb-9951-f456cf753ea1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker.workflow.pipeline import Pipeline\n",
    "\n",
    "from sagemaker import get_execution_role\n",
    "region = sagemaker.Session().boto_region_name\n",
    "role = get_execution_role()\n",
    "sess = sagemaker.session.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d959cb5f-fb2d-48fa-bb72-5250d6a018cc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "# Replace 'your-bucket-name' with your desired bucket name\n",
    "bucket_name = 'sagemaker-etl-prod'\n",
    "\n",
    "def create_s3_bucket(bucket_name, region='ap-southeast-1'):\n",
    "    try:\n",
    "        s3_client = boto3.client('s3', region_name=region)\n",
    "        s3_client.create_bucket(Bucket=bucket_name, CreateBucketConfiguration={'LocationConstraint': region})\n",
    "        print(\"Bucket created successfully.\")\n",
    "    except s3_client.exceptions.ClientError as e:\n",
    "        if e.response['Error']['Code'] == 'BucketAlreadyOwnedByYou':\n",
    "            print(\"Bucket already exists and is owned by you.\")\n",
    "        else:\n",
    "            print(\"An error occurred:\", e)\n",
    "            \n",
    "# Check and create the bucket\n",
    "create_s3_bucket(bucket_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e89ad0f-ac52-4c2f-bd49-d40dc00bcc8c",
   "metadata": {},
   "source": [
    "## Processing folder & env scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c3d8ff8-e851-4d50-a331-8a618537b9bd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!mkdir src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e41a6bdf-161f-4b46-ae7c-13cb6ecd9349",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile src/__init__.py\n",
    "\n",
    "# You can leave this file empty or write any desired content if needed.\n",
    "# For example, you can define package-level attributes, import submodules, etc.\n",
    "# If you don't need any specific content, you can leave the file empty.\n",
    "\n",
    "# Content of __init__.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "874cb260-5d61-4bca-a5fa-ad2a8f2d652a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile src/requirements.txt\n",
    "\n",
    "psycopg2-binary  \n",
    "pyathena\n",
    "boto3\n",
    "openpyxl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72f8041d-db5b-4790-bb33-503060fb7fd0",
   "metadata": {},
   "source": [
    "## Dependency scripts - SQL_dict.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1884ca38-479f-4cde-96a6-bcdebccd8db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile src/SQL_dict.py\n",
    "\n",
    "query_BH_OLTP = '''SELECT * FROM BH_OLTP'''\n",
    "query_C_OLAP = '''SELECT * FROM C_OLAP'''\n",
    "query_Bounty_OLAP = '''SELECT * FROM Bounty_OLAP'''\n",
    "query_Bounty_OLTP = '''SELECT * FROM Bounty_OLTP'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9693ecde-b2c1-47ae-b704-9aac52d809c5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%cp src/SQL_dict.py SQL_dict.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa31cf42-2d3d-4d3e-b5f3-39b00ba4ceea",
   "metadata": {},
   "source": [
    "## Processing script - preprocessing.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d1f9978-1bb4-49b6-b5bd-8694010fed29",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install -r src/requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "346aa8ff-8b56-4696-89b9-c9b339041a3c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile src/preprocessing.py\n",
    "\n",
    "#! /usr/bin/env python\n",
    "\n",
    "import psycopg2  # pip install psycopg2-binary\n",
    "from pyathena import connect\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from urllib.parse import urlparse\n",
    "import re\n",
    "\n",
    "# Ignore all warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import boto3\n",
    "s3 = boto3.client(\"s3\")\n",
    "session = boto3.session.Session()\n",
    "region_name = session.region_name\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "now = datetime.now()\n",
    "today_date = now.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "# import custom repo python script\n",
    "import SQL_dict\n",
    "import importlib\n",
    "importlib.reload(SQL_dict)\n",
    "\n",
    "\n",
    "def RDS_extraction(query):\n",
    "    ### prod\n",
    "    # Set the connection parameters\n",
    "    dbname = 'dbname'\n",
    "    host = 'hostname-prod-psql.cluster-xxxxxxxxxxxx.ap-southeast-1.rds.amazonaws.com'\n",
    "    port = '5432' # Default PostgreSQL port\n",
    "    user = 'user'\n",
    "    password = 'password'\n",
    "\n",
    "    print('conn start')\n",
    "    # Connect to the database\n",
    "    conn = psycopg2.connect(dbname=dbname, host=host, port=port, user=user, password=password)\n",
    "\n",
    "    # Open a cursor to perform database operations\n",
    "    cur = conn.cursor() # to be tested (src/extraction.py:37:12: W0621: Redefining name 'df_endpoint' from outer scope (line 400)(redefined-outer-name))\n",
    "\n",
    "    print('rollback')\n",
    "    conn.rollback()\n",
    "\n",
    "    print('read_sql_query')\n",
    "    df_endpoint = pd.read_sql(query, conn)\n",
    "\n",
    "    return df_endpoint\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--input-path\", type=str, default=\"/opt/ml/processing\")\n",
    "    args, _ = parser.parse_known_args()\n",
    "    base_dir = args.input_path\n",
    "    df = pd.read_csv(\n",
    "        f\"{base_dir}/input/bank-additional-full.csv\",\n",
    "        header=0\n",
    "    )\n",
    "\n",
    "    df_BH_OLAP = RDS_extraction(SQL_dict.query_BH_OLAP)\n",
    "    df_C_OLAP = RDS_extraction(SQL_dict.query_C_OLAP)\n",
    "    df_Bounty_OLAP = RDS_extraction(SQL_dict.query_Bounty_OLAP)\n",
    "    df_Bounty_OLTP = RDS_extraction(SQL_dict.query_Bounty_OLTP)\n",
    "    \n",
    "    ############################################################################ \n",
    "    try:\n",
    "        os.makedirs(f\"/opt/ml/processing/output/\")\n",
    "        print(\"Successfully created directories\")\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(\"Could not make directories\")\n",
    "        #pass \n",
    "    ############################################################################ \n",
    "    try:\n",
    "        os.makedirs(f\"/opt/ml/processing/output/logs/\")\n",
    "        print(\"Successfully created directories\")\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(\"Could not make directories\")\n",
    "        #pass \n",
    "        \n",
    "    ############################################################################    \n",
    "    try:\n",
    "        print('CSV exporting')  \n",
    "        df_BH_OLAP.to_csv(f'/opt/ml/processing/output/df_BH_OLAP.csv', index=False)\n",
    "        df_BH_OLAP.to_csv(f'/opt/ml/processing/output/logs/df_BH_OLAP_{today_date}.csv', index=False)\n",
    "\n",
    "        df_C_OLAP.to_csv(f'/opt/ml/processing/output/df_C_OLAP.csv', index=False)\n",
    "        df_C_OLAP.to_csv(f'/opt/ml/processing/output/logs/df_C_OLAP_{today_date}.csv', index=False)\n",
    "        \n",
    "        df_Bounty_OLAP.to_csv(f'/opt/ml/processing/output/df_Bounty_OLAP.csv', index=False)\n",
    "        df_Bounty_OLAP.to_csv(f'/opt/ml/processing/output/logs/df_Bounty_OLAP_{today_date}.csv', index=False)     \n",
    "        \n",
    "        df_Bounty_OLTP.to_csv(f'/opt/ml/processing/output/df_Bounty_OLTP.csv', index=False)\n",
    "        df_Bounty_OLTP.to_csv(f'/opt/ml/processing/output/logs/df_Bounty_OLTP_{today_date}.csv', index=False)      \n",
    "        \n",
    "        print(\"Wrote CSV files successfully\")\n",
    "\n",
    "        # DataFrames and corresponding sheet names\n",
    "        data_frames = {\n",
    "            \"BH_OLAP\": df_BH_OLAP,\n",
    "            \"B_OLAP\": df_Bounty_OLAP,\n",
    "            \"B_OLTP\": df_Bounty_OLTP,\n",
    "            \"C_OLAP\": df_C_OLAP,\n",
    "        }\n",
    "\n",
    "        excel_file = f'/opt/ml/processing/output/summary_data.xlsx'\n",
    "        \n",
    "        # Write to Excel\n",
    "        with pd.ExcelWriter(excel_file) as writer:\n",
    "            for sheet_name, df in data_frames.items():\n",
    "                df.to_excel(writer, sheet_name=sheet_name, index=False)\n",
    "        print(\"Wrote XLSX files successfully\") \n",
    "        \n",
    "    except Exception as e:\n",
    "        print(\"Failed to write the files\")\n",
    "        print(e)\n",
    "        #pass\n",
    "    ############################################################################  \n",
    "    \n",
    "    print(\"Completed running the processing job\")\n",
    "\n",
    "    ################################### SNS #########################################  \n",
    "    # Define S3 file URL\n",
    "    s3_file_url = 'https://s3.console.aws.amazon.com/s3/buckets/sagemaker-etl-prod?region=ap-southeast-1&bucketType=general&tab=objects'\n",
    "    \n",
    "    # Publish message to SNS topic\n",
    "    sns_topic_arn = 'arn:aws:sns:ap-southeast-1:963727426434:milton'\n",
    "\n",
    "    # Create SNS client\n",
    "    sns_client = boto3.client('sns', region_name='ap-southeast-1')\n",
    "\n",
    "    # Create the message containing the S3 file URL\n",
    "    message = f\"Daily ETL transformation done. Logs Date: {today_date}. Please find the file from S3 at the following URL: {s3_file_url}\"\n",
    "\n",
    "    # Publish message to SNS topic\n",
    "    sns_client.publish(\n",
    "        TopicArn=sns_topic_arn,\n",
    "        Message=message,\n",
    "        Subject='S3 File Notification'\n",
    "    )\n",
    "\n",
    "    print('Message published to SNS topic.')\n",
    "    \n",
    "    \n",
    "    ################################### SES ######################################### \n",
    "    import base64\n",
    "    \n",
    "    S3_BUCKET='sagemaker-etl-prod'\n",
    "    S3_FILE='summary_data.xlsx'\n",
    "    S3_REGION='ap-southeast-1'\n",
    "    SES_REGION='ap-southeast-1'\n",
    "    SENDER_EMAIL='milton@domain.tech'\n",
    "    DESTINATION_EMAILS='milton@domain.tech'\n",
    "    \n",
    "    # Create an S3 client\n",
    "    s3_client = boto3.client('s3', region_name=S3_REGION)\n",
    "\n",
    "    # Create an SES client\n",
    "    ses_client = boto3.client('ses', region_name=SES_REGION)\n",
    "\n",
    "    # Step 1: Retrieve the file from S3\n",
    "    s3_object = s3_client.get_object(Bucket=S3_BUCKET, Key=S3_FILE)\n",
    "    file_content = s3_object['Body'].read()\n",
    "    \n",
    "    # Step 2: Encode the file content\n",
    "    file_content_encoded = base64.b64encode(file_content).decode('utf-8')\n",
    "\n",
    "    # Step 3: Use the AWS SDK to send the email\n",
    "    raw_email = (\n",
    "        f\"From: {SENDER_EMAIL}\\n\"\n",
    "        f\"To: {DESTINATION_EMAILS}\\n\"\n",
    "        f\"Subject: Daily Summary Data\\n\"\n",
    "        f\"MIME-Version: 1.0\\n\"\n",
    "        f\"Content-Type: multipart/mixed; boundary=\\\"NextPart\\\"\\n\\n\"\n",
    "        f\"--NextPart\\n\"\n",
    "        f\"Content-Type: text/plain\\n\\n\"\n",
    "        f\"Attachment files from Sagemaker ETL\\n\\n\"\n",
    "        f\"--NextPart\\n\"\n",
    "        f\"Content-Type: application/vnd.openxmlformats-officedocument.spreadsheetml.sheet\\n\"\n",
    "        f\"Content-Disposition: attachment; filename=\\\"summary_data.xlsx\\\"\\n\"\n",
    "        f\"Content-Transfer-Encoding: base64\\n\\n\"\n",
    "        f\"{file_content_encoded}\\n\\n\"\n",
    "        f\"--NextPart--\"\n",
    "    )\n",
    "\n",
    "    email_params = {\n",
    "        'Source': SENDER_EMAIL,\n",
    "        'Destinations': DESTINATION_EMAILS.split(','),\n",
    "        'RawMessage': {'Data': raw_email}\n",
    "    }\n",
    "\n",
    "    response = ses_client.send_raw_email(**email_params)\n",
    "\n",
    "    print('File sent!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f29e13-1a1b-4814-903d-1ae72af08f93",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.network import NetworkConfig\n",
    "\n",
    "# network_config object (please change the information to your own security groups and subnets)\n",
    "security_group_ids = [\"sg-xxxxxxxxxxxxxxxxx\"]\n",
    "subnets =  [ \"subnet-xxxxxxxxxxxxxxxxx\", \"subnet-xxxxxxxxxxxxxxxxx\", \"subnet-xxxxxxxxxxxxxxxxx\"]\n",
    "\n",
    "network_config = NetworkConfig(enable_network_isolation=False, security_group_ids=security_group_ids, subnets=subnets)\n",
    "\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.pytorch.processing import PyTorchProcessor\n",
    "\n",
    "pytorch_processor = PyTorchProcessor(\n",
    "    role=role,\n",
    "    instance_type=\"ml.m5.xlarge\",\n",
    "    instance_count=1,\n",
    "    framework_version=\"1.13\",\n",
    "    py_version=\"py39\",\n",
    "    network_config = network_config \n",
    ")\n",
    "\n",
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "\n",
    "input_filepath_inject = 's3://sagemaker-lighthouse-log-prod/external/injecion_cheatsheet.csv'\n",
    "output_path_ai_h1s1 = 's3://sagemaker-etl-prod'\n",
    "\n",
    "classify_inputs = [\n",
    "    ProcessingInput(\n",
    "        source=input_filepath_inject,\n",
    "        destination=\"/opt/ml/processing/inject\"\n",
    "    ),\n",
    "]\n",
    "\n",
    "classify_outputs= [\n",
    "    ProcessingOutput(\n",
    "        output_name=f\"output_ETL\",  # task name\n",
    "        source=\"/opt/ml/processing/output\",\n",
    "        destination=output_path_ai_h1s1\n",
    "    ), \n",
    "]\n",
    "\n",
    "classify_arguments = [\n",
    "    \"--file-name\", \"reviews.tsv.gz\",\n",
    "    \"--model-name\", \"distilbert-base-uncased\",\n",
    "    \"--train-ratio\", \"0.7\",\n",
    "    \"--val-ratio\", \"0.15\",\n",
    "    \"--star-threshold\", \"4\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a9a9f05-0e69-4a6f-ae34-4a0a182de6cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fast running\n",
    "pytorch_processor.run(code=\"preprocessing.py\",\n",
    "                      source_dir=\"scripts\", # add processing.py and requirements.txt here\n",
    "                      inputs=classify_inputs,\n",
    "                      outputs=classify_outputs,\n",
    "                      arguments=classify_arguments\n",
    "                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c212d66-676d-4233-b244-96f2f5fb74e8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "from sagemaker.workflow.steps import ProcessingStep\n",
    "\n",
    "###################################################################################################################\n",
    "# frameworkprocessor github issue for 3rd party python packages\n",
    "# https://github.com/aws/sagemaker-python-sdk/issues/2656\n",
    "\n",
    "from sagemaker.processing import FrameworkProcessor  # or change with any other FrameworkProcessor like HuggingFaceProcessor\n",
    "from sagemaker.pytorch.processing import PyTorch, PyTorchProcessor\n",
    "from sagemaker.workflow.pipeline_context import PipelineSession\n",
    "from sagemaker.network import NetworkConfig\n",
    "from sagemaker.workflow.pipeline import Pipeline\n",
    "from sagemaker.workflow.steps import ProcessingStep\n",
    "\n",
    "session = PipelineSession()\n",
    "\n",
    "processor = FrameworkProcessor(\n",
    "    estimator_cls=PyTorch,\n",
    "    framework_version='1.13',\n",
    "    role = get_execution_role(),\n",
    "    instance_count=1,\n",
    "    # instance_type=\"ml.m5.4xlarge\",\n",
    "    instance_type=\"ml.m5.xlarge\",\n",
    "    sagemaker_session = session,\n",
    "    py_version=\"py39\",\n",
    "    network_config = network_config \n",
    ")\n",
    "\n",
    "\n",
    "###################################### - 2nd step - feature engineering\n",
    "\n",
    "classify_args = processor.run(\n",
    "    code='preprocessing.py',\n",
    "    source_dir=\"src\", # add processing.py and requirements.txt here\n",
    "    inputs=classify_inputs,\n",
    "    outputs=classify_outputs,\n",
    "    arguments=classify_arguments\n",
    ")\n",
    "\n",
    "classify_process = ProcessingStep(\n",
    "    name=\"ETL_daily\",\n",
    "    step_args=classify_args\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a39295d-83e3-4086-9cb4-3ec84a6635ba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.workflow.pipeline import Pipeline\n",
    "\n",
    "pipeline_name = \"ETL-daily\"\n",
    "\n",
    "pipeline = Pipeline(\n",
    "    name=pipeline_name,\n",
    "#     parameters=[\n",
    "#         input_data,\n",
    "#         processing_instance_type, \n",
    "#         processing_instance_count,\n",
    "#         processed_data,\n",
    "#     ],\n",
    "    steps=[\n",
    "        # extraction_process, \n",
    "        classify_process,\n",
    "        # reporting_process, \n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9918b81-a018-4622-8506-0cb939aa211d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "definition = json.loads(pipeline.definition())\n",
    "definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc36d3f7-63bb-4405-bdb5-68c60058cba5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pipeline.upsert(role_arn=role)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96bc84c6-5b19-407b-ab74-4fd3d7059cd3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "execution = pipeline.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7905610a-93a8-44cb-9a70-3a194ea04af5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "\n",
    "ex_desc = execution.describe()\n",
    "status = ex_desc['PipelineExecutionStatus']\n",
    "print(status+ \" \", end=\"\")\n",
    "\n",
    "while status == 'Executing':\n",
    "    status = execution.describe()['PipelineExecutionStatus']\n",
    "    print(\".\", end=\"\")\n",
    "    sleep(10)\n",
    "    \n",
    "print(f\"\\n{status}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c51d4af-f056-4b25-beca-f4a4c5b70ac1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "538958fc-050a-4d2d-be04-52b7a8be3413",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "219925a2-d60d-4bcc-b4c4-3c5fbdbf1183",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f75bfe-db61-42be-aad0-9431bbe0f69f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
